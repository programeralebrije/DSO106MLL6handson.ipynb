{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Collecting package metadata (current_repodata.json): done\n",
    "Solving environment: done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All requested packages already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "Type Markdown and LaTeX:  ùõº2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [2]:\n",
    "xxxxxxxxxx\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Text\n",
    "‚Äã\n",
    "Type Markdown and LaTeX:  ùõº2\n",
    "\n",
    "In [4]:\n",
    "xxxxxxxxxx\n",
    " \n",
    "url = 'https://www.gutenberg.org/files/11/11-h/11-h.htm'\n",
    "In [5]:\n",
    "xxxxxxxxxx\n",
    "r = requests.get(url)\n",
    "‚Äã\n",
    "In [6]:\n",
    "xxxxxxxxxx\n",
    "type(r)\n",
    "‚Äã\n",
    "Out[6]:\n",
    "requests.models.Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Text to Soup\n",
    "‚Äã\n",
    "Type Markdown and LaTeX: ùõº2\n",
    "In [7]:\n",
    "xxxxxxxxxx\n",
    "html = r.text\n",
    "soup = BeautifulSoup(html,\"html.parser\")\n",
    "type(soup)\n",
    "Out[7]:\n",
    "bs4.BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use HTML Tags to Extract Useful Info\n",
    "‚Äã\n",
    "Type Markdown and LaTeX: ùõº2\n",
    "In [8]:\n",
    "xxxxxxxxxx\n",
    " \n",
    "soup.title.string\n",
    "‚Äã\n",
    "Out[8]:\n",
    "'The Project Gutenberg eBook of Alice‚Äôs Adventures in Wonderland, by Lewis Carroll'\n",
    " \n",
    "# Tokenize Data\n",
    "‚Äã\n",
    "Type Markdown and LaTeX: ùõº2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [9]:\n",
    "xxxxxxxxxx\n",
    "text = soup.get_text()\n",
    "tokenizer = RegexpTokenizer('\\w+')\n",
    "tokens = tokenizer.tokenize(text)\n",
    "tokens[:5]\n",
    "Out[9]:\n",
    "['The', 'Project', 'Gutenberg', 'eBook', 'of']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Capitalization\n",
    "‚Äã\n",
    "Type Markdown and LaTeX: ùõº2\n",
    "In [10]:\n",
    "xxxxxxxxxx\n",
    "words = []\n",
    "for word in tokens:\n",
    "    words.append(word.lower())\n",
    "In [11]:\n",
    "words[:5]\n",
    "‚Äã\n",
    "Out[11]:\n",
    "['the', 'project', 'gutenberg', 'ebook', 'of']\n",
    " \n",
    "# Remove Stopwords\n",
    "‚Äã\n",
    "Type Markdown and LaTeX: ùõº2\n",
    "In [19]:\n",
    "xxxxxxxxxx\n",
    "nltk.download('stopwords')\n",
    "[nltk_data] Downloading package stopwords to\n",
    "[nltk_data]     /Users/adilarrazolo/nltk_data...\n",
    "[nltk_data]   Unzipping corpora/stopwords.zip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Out[19]:\n",
    "True\n",
    "In [20]:\n",
    "xxxxxxxxxx\n",
    " \n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "‚Äã\n",
    "In [21]:\n",
    "xxxxxxxxxx\n",
    "stopwords[:10]\n",
    "‚Äã\n",
    "Out[21]:\n",
    "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n",
    "In [22]:\n",
    "xxxxxxxxxx\n",
    " \n",
    "wordsWithoutStops = []\n",
    "for word in words:\n",
    "    if word not in stopwords:\n",
    "        wordsWithoutStops.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [23]:\n",
    "xxxxxxxxxx\n",
    " \n",
    "wordsWithoutStops[:5]\n",
    "‚Äã\n",
    "Out[23]:\n",
    "['project', 'gutenberg', 'ebook', 'alice', 'adventures']\n",
    "In [24]:\n",
    "xxxxxxxxxx\n",
    " \n",
    "sns.set()\n",
    "frequencyDis = nltk.FreqDist(wordsWithoutStops)\n",
    "frequencyDis.plot(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Out[24]:\n",
    "<matplotlib.axes._subplots.AxesSubplot at 0x7fd14e111250>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
